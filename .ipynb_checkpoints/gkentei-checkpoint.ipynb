{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 人工知能(AI)とは\n",
    "\n",
    "# 「人工知能」\n",
    "# 1956年 ダートマス会議　ジョン・マッカーシー\n",
    "# 計算機による知的な情報処理システムの設計や実現\n",
    "\n",
    "# 「機械学習」\n",
    "# 人間が自然に行なっている学習能力と同等の機能をコンピューターで実現しようとする技術・手法\n",
    "# アーサー・サミュエル「明示的にプログラムしなくても学習する能力をコンピューターに与える研究分野」\n",
    "\n",
    "# 「深層学習」\n",
    "# ディープニューラルネットワークを用いて、学習、予測を行うモデル\n",
    "# ILSVRC 2012年\n",
    "\n",
    "# 世界初の汎用コンピューターの誕生\n",
    "# 1946年 エニアック(ENIAC)\n",
    "\n",
    "# 第1次AIブーム(推論,探索の時代：1950年代後半~1960年代)\n",
    "# 内容：迷路や数学の定理のような簡単な問題「トイプロブレム(おもちゃの問題)」\n",
    "# 失速：複雑な現実問題は解けない。\n",
    "\n",
    "# 第2次AIブーム(知識の時代：1980年代)\n",
    "# 内容：エキスパートシステム, 知識ベースと推論エンジンを使って推論を行う\n",
    "# 失速：知識を蓄積,管理することの大変さが明らかに。\n",
    "# 日本では政府によって「第５世代コンピュータ」\n",
    "\n",
    "# 第3次AIブーム(機械学習,特徴表現学習の時代：2010年代~)\n",
    "# 内容：ビックデータを用いることで、人工知能が自ら知識を獲得する機械学習が実用化。\n",
    "#          特徴量を人工知能がみずから取得するディープラーニング(深層学習)の登場\n",
    "\n",
    "# チューリングテスト\n",
    "# ELIZAとPARRYのRFC439\n",
    "\n",
    "# シンギュラリティ\n",
    "# レイ・カーツワイル　「シンギュラリティは2045年に到来する」\n",
    "# ヒューゴ・デ・ガリス 「人工知能は人間の知能の1兆の1兆倍になる」\n",
    "# イーロン・マスク　OpenAIの創立\n",
    "# オレン・エツィオーニ 「馬鹿げている」\n",
    "# ヴァーナー・ヴィンジ 「機械が人間の役に立つふりをしなくなること」\n",
    "# スティーブン・ホーキング 「AIの完成は人類の終焉を意味するかもしれない」"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C2 人工知能をめぐる動向\n",
    "\n",
    "# 幅優先探索\n",
    "# 最短距離でゴールに辿り着く解、メモリ不足\n",
    "\n",
    "# 深さ優先探索\n",
    "# 解が最短距離とは限らない、メモリはあまりいらない\n",
    "\n",
    "# 「プランニング」\n",
    "# ロボットの行動計画も探索を利用して作成\n",
    "# STRIPS(前提条件、行動、結果)\n",
    "\n",
    "# SHRDLU(積木の世界を動かす)\n",
    "# 1970年、テリー・ウィノグラード\n",
    "\n",
    "# 組み合わせ\n",
    "# オセロ < チェス < 将棋 < 囲碁\n",
    "\n",
    "# ヒューリスティックな知識\n",
    "# 探索に利用する経験的な知識\n",
    "\n",
    "# モンテカルロ法\n",
    "# コンピュータ同士がランダムに挿し続けプレイアウトさせる\n",
    "# プルートフォース(力任せ)\n",
    "\n",
    "# イライザ(ELIZA)\n",
    "# 人工知能の元祖,1964年,ジョゼフ・ワイゼンバウム\n",
    "# 「イライザ効果」 本物の人間と対話しているような錯覚\n",
    "\n",
    "# エキスパートシステム\n",
    "# マイシン(MYCIN) 血中バクテリアの診断支援\n",
    "# DENDRAL 未知の有機化合物を特定する\n",
    "\n",
    "# オントロジー(概念体系を記述するための方法)\n",
    "# Cycプロジェクト　すべての一般常識をコンピュータに取り込む\n",
    "# 推移律 A=B,B=C A=C\n",
    "# ヘビーウェイトオントロジー Cycプロジェクト\n",
    "# ライトウェイトオントロジー ウェブマイニング、データマイニング、ワトソン\n",
    "\n",
    "# 統計的自然言語処理\n",
    "# 複数の単語をひとまとまりにした単位(句または文単位)\n",
    "# 対訳データ(コーバス)\n",
    "\n",
    "# 自己符号化処理\n",
    "# 入力したものと同じものを出力するように学習する\n",
    "\n",
    "# ILSVRC(画像認識コンペ)\n",
    "# 2012年,Super Vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C3 人工知能分野の問題\n",
    "\n",
    "# フレーム問題\n",
    "# 今しようとしていることに関係のある事柄だけを選び出すことが、実は非常に難しい。\n",
    "\n",
    "# ローブナーコンテスト\n",
    "# チューリングテストに合格する会話ソフトウェアを目指すコンテスト\n",
    "\n",
    "# 「弱いAI」と「強いAI\n",
    "# ジョン・サール\n",
    "# 「強いAI」 本物の心を持つ人工知能はコンピュータで再現できる、ロジャー・ペンローズ「強いAI」は実現できない\n",
    "# 「弱いAI」 便利な道具、「中国語の部屋」\n",
    "\n",
    "# シンボルグランティング問題\n",
    "# スティーブン・ハルナッド、「記号とその対象がいかにして結びつくか」\n",
    "\n",
    "# 知識獲得のボトルネック=「コンピューターが知識獲得することの難しさ」\n",
    "# 1970年代後半、ルールベース翻訳\n",
    "# 1990年~ 統計的翻訳、意味は理解していない\n",
    "# 2016年　ニューラル機械翻訳\n",
    "\n",
    "# シンギュラリティー　「人工知能が人間よりも賢くなる年」\n",
    "# レイ・カーツワイル\n",
    "# 2029 「人工知能が人間より賢くなる」、2045　「シンギュラリティー(技術的特異点)」　特異点= ある基準が適用できなくなる点\n",
    "# スティーブン・ホーキンス 「人類の終焉」\n",
    "# イーロン・マスク 「悪魔を呼び出すことになる」\n",
    "# Google 2014年　倫理委員会"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C4 機械学習の具体的な手法\n",
    "\n",
    "# 回帰\n",
    "# 線形回帰\n",
    "# データの分布があった時にデータに最も当てはまる直線を考える\n",
    "# 重回帰分析を行う場合は「多重共線性の検出」も重要\n",
    "# 相関関係が高い特徴量の組を同時に説明変数を選ぶと、予測が上手くいかなくなる現象\n",
    "# 相関関係=「相関の正負と強さ」\n",
    "# 「線形分離可能」=パーセプトロンを使って解ける問題\n",
    "\n",
    "# 分類\n",
    "# サポートベクタマシン 「マージンの最大化」\n",
    "# 「スラック変数」　誤分類を許容するための工夫\n",
    "# 「カーネル法(カーネル関数)」 線形分離可能でないデータに対応\n",
    "# 「カーネルトリック」 計算量を現実的に抑えつつ、非線形分離を可能にする\n",
    "\n",
    "# 決定木\n",
    "# 不純度が最も減少(情報利得が最も増加)するように条件分岐を作りデータを振り分ける\n",
    "# 「不純度」 = クラスの混じり具合 ジニ係数、エントロピー\n",
    "\n",
    "# ランダムフォレスト\n",
    "# 決定木＋バギング(アンサンブル学習、平均の多数決をとる)\n",
    "# 「バギング」　全体から一部のデータを用いて複数のモデルを用いて学習する、複数のモデルを一気に並列に作成する\n",
    "# 「ブートストラップサンプリング」 それぞれの決定木に対してランダムに一部のデータを取り出して学習に用いる\n",
    "# データの前処理が少なくて済む\n",
    "# 安定して良い精度が出る\n",
    "\n",
    "# ブースティング\n",
    "# 一部のデータを繰り返し抽出し、複数のモデルに学習させる、逐次的に作成→ランダムフォレストより良い結果が出るが、学習に時間がかかる\n",
    "# AdaBoost、勾配ブースティング、XgBoostなどが有名\n",
    "\n",
    "# ロジスティック回帰(分類のこと)\n",
    "# ①対数オッズを重回帰分析により予測\n",
    "# ロジット変換をすることで出力値が0から１の間の値に正規化\n",
    "# ②対数オッズをロジスティック回帰(シグモイド関数、(0,0.5)の値を取る)で変換することで確率の予測値を求める\n",
    "# ③各クラスの確率を計算し、最大確率を実現するクラスがデータが属するクラスと予測\n",
    "# たくさんの種類を分類する時はシグモイド関数ではなく、ソフトマックス関数を使用する。\n",
    "# ニューラルネットワークの１種\n",
    "\n",
    "# 正則化　過学習を予測するための手法\n",
    "# 標準化　平均0,標準偏差1\n",
    "# 正規化 データを0~1にスケーリング\n",
    "\n",
    "# kNN(k Nearest Neighbor)法\n",
    "# データから近い順にk個のデータを見て、それらの多数決によって所属するという方法によりクラス分類が行われる\n",
    "# データの数に偏りがあると、判定結果が不正確になる\n",
    "\n",
    "# k-means\n",
    "# 元のデータからグループ構造を見つけ出し、それぞれをまとめる\n",
    "# クラスタ分析\n",
    "\n",
    "# 主成分分析\n",
    "# 次元削減\n",
    "\n",
    "# 強化学習\n",
    "# エージェントの目的は収益を最大化する方策を獲得すること\n",
    "# 行動を選択することで状態が変化\n",
    "# Deep Q-Network\n",
    "# 価値関数の計算を近似計算するディープニューラルネットワーク、ディープマインド社が作成\n",
    "\n",
    "# データの前処理\n",
    "# 名寄せ(表記の揺れの統一)\n",
    "# 正規化(データの大きさをスケーリング)\n",
    "# 標準化(平均0、標準偏差1)\n",
    "# 基礎統計(データの代表値の計算、散布図にプロット、相関行列)\n",
    "\n",
    "# 評価指標\n",
    "# 正解率　TP+TN/(TP+TN+FP+FN)\n",
    "# 適合率 TP/(TP+FP) 予測が正の中で、実際に正であったもの\n",
    "# 再現率 TP/(TP+FN) 実際に正であるものの中で、正だと予測できたもの\n",
    "# F値 2*P*R/(P+R)\n",
    "\n",
    "# 正則化\n",
    "# 学習の際に用いる式に功を追加することによってとりうる重みの値の範囲を制限し、角に重みが訓練データに対してのみ防ぐ役割。\n",
    "# アンダーフィッティング(正則化しすぎてしまうこと)\n",
    "# L1正則化(ラッソ回帰) 一部のパラメータの値をゼロにすることで、特徴選択を行うことができる\n",
    "# L2 正則化(リッジ回帰)　パラメータの大きさに応じてゼロに近づけることで、汎化された滑らかなモデルを得ることができる。\n",
    "# 合わせてElastic Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 線形代数\n",
    "# ベクトルや行列を扱う分野\n",
    "\n",
    "# 記述統計\n",
    "# 手元のデータ分析を行う\n",
    "\n",
    "# 推計統計\n",
    "# 手元のデータの背後にある母集団の性質を予測する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C5 ディープラーニングの概要\n",
    "\n",
    "# 「ディープラーニング」\n",
    "# 非線形分類ができる\n",
    "# 隠れ層を増やしたニューラルネットワーク\n",
    "# 欠点\n",
    "# 過学習、調整するパラメータ数が多い、勾配消失問題\n",
    "\n",
    "# 「勾配焼失問題」\n",
    "# 隠れ層を遡るごとに(活性化関数の微分が掛け合わさる)伝搬していく誤差はどんどん小さくなっていく\n",
    "\n",
    "# 「勾配降下法」\n",
    "# 重みを少しずつ更新して勾配が最小になる点を探索\n",
    "# 局所最適解\n",
    "# その周辺では誤差の値は小さいが、最小値ではない\n",
    "# 大域的最適解\n",
    "# 誤差の値を最も小さくする解\n",
    "# 停留点\n",
    "# 局所最適解でも大域最適解でもないが勾配が0になる点\n",
    "# 鞍点\n",
    "# 停留点のうちある方向から見るとごく承知だが、別の方向から見ると極大値\n",
    "# ディープラーニングは局所最適解を求めることを目的とする\n",
    "\n",
    "# 「オートエンコーダ」(自己符号化器)\n",
    "# 2006年、ジェフリー・ヒントン\n",
    "# 可視層(入力層と出力層)と隠れ層の2層からなるネットワーク\n",
    "# 入力と出力が同じになるようなニューラルネットラーク　→　隠れ層には「入力の情報が圧縮されたもの」\n",
    "# 「エンコード」(入力層→隠れ層)、「デコード」(隠れ層→出力層)\n",
    "\n",
    "# 「ディープオートエンコーダ」(積層オートエンコーダ) = 事前学習＋ファインチューニング\n",
    "# オートエンコーダを順番に学習させ積み重ねていく(事前学習)\n",
    "# 入力層に近い層から順番に学習させる　→　逐次的(順番に学習していく)\n",
    "\n",
    "# 「ファインチューニング」(最後の仕上げ、ネットワーク全体に対する教師あり学習)\n",
    "# 最後にロジスティック回帰(シグモイド関数もしくはソフトマックス関数による出力層)を足す。→線形回帰層を足す。\n",
    "\n",
    "# 「深層信念ネットワーク」\n",
    "# 教師なし学習(オートエンコーダに相当する層)に制限付きボルツマンマシン\n",
    "\n",
    "# ハードウェアの進歩\n",
    "# 「半導体の性能と集積は、18ヶ月ごとに2倍になる」\n",
    "\n",
    "# CPU(Central Processing Unit)\n",
    "# コンピューター全般の作業を処理する役割\n",
    "\n",
    "# GPU(Graphics Processing Unit)\n",
    "# 画像処理に関する演算\n",
    "\n",
    "# GPGPU(General-Purpose computing on GPU)\n",
    "# 画像以外の目的での使用に最適化されたGPU\n",
    "\n",
    "# NVIDIA社\n",
    "# GoogleはTPU(Tensor Processing Unit)\n",
    "\n",
    "# バーニーおじさんのルール\n",
    "# 「モデルのパラメータ数の10倍のデータ数が必要」\n",
    "# みにくいアヒルの子定理\n",
    "# 機械学習の定式化によって「普通のアヒル」と「みにくいアヒル」の区別はできない\n",
    "# モラベックスのパラドックス\n",
    "# 機械にとって高度な推論よりも1歳児レベルの知恵や運動スキルを身につける方が遥かに難しい\n",
    "\n",
    "# 単純パーセプトロン = ステップ関数\n",
    "# ReLU関数　= 勾配消失問題が起きにくい、現在主流\n",
    "# ソフトマックス関数 = 確率\n",
    "\n",
    "# エポック\n",
    "# 訓練データを何度学習に用いたか\n",
    "# イテレーション\n",
    "# 重みを何度更新したか\n",
    "\n",
    "# 逐次学習(確率的勾配降下法)\n",
    "# 訓練データ1つに対して、重みを1回更新する\n",
    "\n",
    "# ミニバッチ学習\n",
    "# ミニバッチ...いくつかの訓練データからランダムにサンプリングした小さなデータの集まり\n",
    "# ミニバッチに含まれるデータ全てについて誤差の総和を計算し、その総和を小さくするように重みを1回更新する(ミニバッチ勾配降下法)\n",
    "\n",
    "# バッチ学習 = 勾配降下法、バッチ勾配降下法\n",
    "# 訓練データ全ての誤差を計算し、重みを1回更新する(イテレーションとエポックが等しい)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
